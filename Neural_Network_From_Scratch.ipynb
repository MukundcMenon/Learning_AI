{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOo8UqDe7Fa5An/WQGchPl+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MukundcMenon/Learning_AI/blob/main/Neural_Network_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlYCnsKdxdJm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "inputs = [1.0, 2.0, 3.0, 2.5]\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "biases = [2.0, 3.0, 0.5]\n",
        "\n",
        "# Bare Python Way to get the output of a neuron\n",
        "layer_output = []\n",
        "\n",
        "for weight,bias in zip(weights,biases):\n",
        "  neuron_output = 0\n",
        "  for input,input_weight in zip(inputs,weight):\n",
        "      neuron_output += input * input_weight\n",
        "  layer_output.append(neuron_output+bias)\n",
        "\n",
        "print(layer_output)\n",
        "\n",
        "# Numpy Way of getting an output of a neuron\n",
        "layer_output = np.dot(weights,inputs) + biases\n",
        "print(layer_output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking inputs as batches\n",
        "\n",
        "X = [[1, 2, 3, 2.5],\n",
        "     [2.0, 5.0, -1.0, 2.0],\n",
        "     [-1.5, 2.7, 3.3, -0.8]]\n",
        "\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "biases = [2.0, 3.0, 0.5]\n",
        "layer_output = np.dot(X, np.array(weights).T) + biases\n",
        "layer_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMQqERm53gF_",
        "outputId": "15e8b565-1b20-449b-8e63-eb62af5c2c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4.8  ,  1.21 ,  2.385],\n",
              "       [ 8.9  , -1.81 ,  0.2  ],\n",
              "       [ 1.41 ,  1.051,  0.026]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "Initialise the weights and biases\n",
        "The initialisation should be within a certain numerical range\n",
        "The lower the range, the better the chance to reduce exploding\n",
        "Generally the range chosen is between -1 to 1\n",
        "'''\n",
        "class DenseLayer():\n",
        "  def __init__(self,n_inputs, n_neurons):\n",
        "    self.weights = 0.1 * np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "class Activation_ReLU():\n",
        "  def forward(self,inputs):\n",
        "    self.output = np.maximum(0,inputs)\n",
        "\n",
        "\"\"\"\n",
        "To implement the Activation_Softmax class\n",
        "The exponential should not end up in an exploding problem\n",
        "Hence inidividual numbers of the array are subtracted from thne largest value from the array\n",
        "Then put into e^x and then normalised\n",
        "This results in a value of the normalisation between 0 and 1\n",
        "\"\"\"\n",
        "\n",
        "class Activation_Softmax:\n",
        "  def forward(self, inputs):\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis =1, keepdims=True))\n",
        "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "    self.output = probabilities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "G5MSi5nL7AIA",
        "outputId": "303e6035-5a40-4e94-c4e0-d7912b37fe55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\nX = [[1, 2, 3, 2.5],\\n     [2.0, 5.0, -1.0, 2.0],\\n     [-1.5, 2.7, 3.3, -0.8]]\\n\\nlayer1 = DenseLayer(4,5)\\nlayer2 = DenseLayer(5,2)\\n\\nlayer1.forward(X)\\nprint(layer1.output)\\nlayer2.forward(layer1.output)\\nprint(layer2.output)\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset: A Spiral dataset\n",
        "\n",
        "def spiral_data(points, classes):\n",
        "    X = np.zeros((points*classes, 2))\n",
        "    y = np.zeros(points*classes, dtype='uint8')\n",
        "    for class_number in range(classes):\n",
        "        ix = range(points*class_number, points*(class_number+1))\n",
        "        r = np.linspace(0.0, 1, points)  # radius\n",
        "        t = np.linspace(class_number*4, (class_number+1)*4, points) + np.random.randn(points)*0.2\n",
        "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
        "        y[ix] = class_number\n",
        "    return X, y\n",
        "\n",
        "\n",
        "X, y = spiral_data(100, 3)\n",
        "\"\"\"\n",
        "The above is a dataset containing X and Y axis of a spiral dataset\n",
        "Since it is only two axis, the number of features is 2\n",
        "Hence the n_inputs = 2\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "W6wNDwYwD0BC",
        "outputId": "d038da5d-7864-4f63-fe2e-18c6c62eaf0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe above is a dataset containing X and Y axis of a spiral dataset\\nSince it is only two axis, the number of features is 2\\nHence the n_inputs = 2\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = DenseLayer(2,3)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dense2 = DenseLayer(3,3) # Treating this as an Output layer, and the data has 3 Classes\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "print(activation2.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S87Kg2i5OzlR",
        "outputId": "36cbc7b4-2b78-4a03-8fd3-76aa3534204a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33333333 0.33333333 0.33333333]\n",
            " [0.33333394 0.33330825 0.33335781]\n",
            " [0.33338024 0.33323415 0.33338561]\n",
            " [0.33333558 0.33324088 0.33342354]\n",
            " [0.33333656 0.33320023 0.33346322]\n",
            " [0.33336726 0.33317989 0.33345284]\n",
            " [0.33333791 0.33314417 0.33351793]\n",
            " [0.33333905 0.33309653 0.33356442]\n",
            " [0.33333939 0.33308205 0.33357856]\n",
            " [0.33334043 0.33303859 0.33362098]\n",
            " [0.33334086 0.33302041 0.33363872]\n",
            " [0.33334215 0.33296595 0.33369189]\n",
            " [0.33334287 0.33293556 0.33372157]\n",
            " [0.33334391 0.33289148 0.33376461]\n",
            " [0.33334475 0.33285572 0.33379953]\n",
            " [0.33334528 0.33283304 0.33382168]\n",
            " [0.33334557 0.33282085 0.33383358]\n",
            " [0.33336407 0.33340979 0.33322614]\n",
            " [0.33334563 0.33281799 0.33383638]\n",
            " [0.33334842 0.33269829 0.33395329]\n",
            " [0.33335612 0.33300646 0.33363742]\n",
            " [0.33337147 0.33343482 0.33319371]\n",
            " [0.33336643 0.33319728 0.33343629]\n",
            " [0.33339043 0.33382517 0.3327844 ]\n",
            " [0.33339127 0.33383238 0.33277635]\n",
            " [0.3334498  0.33377016 0.33278003]\n",
            " [0.33339474 0.33386246 0.3327428 ]\n",
            " [0.33338869 0.33377985 0.33283146]\n",
            " [0.33340576 0.33395801 0.33263623]\n",
            " [0.33340724 0.33397091 0.33262184]\n",
            " [0.33335719 0.33231274 0.33433007]\n",
            " [0.33360769 0.33356689 0.33282542]\n",
            " [0.33341601 0.33404705 0.33253694]\n",
            " [0.33341782 0.33406279 0.33251939]\n",
            " [0.33342117 0.33409194 0.33248689]\n",
            " [0.33339798 0.33354831 0.33305371]\n",
            " [0.33405954 0.33248832 0.33345214]\n",
            " [0.33347576 0.33404768 0.33247656]\n",
            " [0.33411907 0.33238086 0.33350006]\n",
            " [0.33439697 0.33135126 0.33425177]\n",
            " [0.33374705 0.33348645 0.3327665 ]\n",
            " [0.3344205  0.33141211 0.3341674 ]\n",
            " [0.33425566 0.33210507 0.33363927]\n",
            " [0.33455439 0.33075067 0.33469494]\n",
            " [0.33443914 0.33153842 0.33402243]\n",
            " [0.33461114 0.33063059 0.33475827]\n",
            " [0.33451771 0.33133853 0.33414377]\n",
            " [0.33425445 0.3323142  0.33343135]\n",
            " [0.33467134 0.33050321 0.33482545]\n",
            " [0.33472198 0.33039606 0.33488196]\n",
            " [0.33458556 0.33131129 0.33410315]\n",
            " [0.33465036 0.3305476  0.33480204]\n",
            " [0.33470969 0.33042207 0.33486824]\n",
            " [0.33483568 0.33015546 0.33500886]\n",
            " [0.33471072 0.33041988 0.3348694 ]\n",
            " [0.33483298 0.33016117 0.33500585]\n",
            " [0.33473716 0.33106326 0.33419958]\n",
            " [0.33494341 0.32992748 0.33512911]\n",
            " [0.33369694 0.33158555 0.33471751]\n",
            " [0.33355965 0.33171769 0.33472265]\n",
            " [0.3347138  0.33041337 0.33487283]\n",
            " [0.33484851 0.3301283  0.33502318]\n",
            " [0.33337768 0.33135833 0.33526398]\n",
            " [0.33374684 0.33141465 0.33483851]\n",
            " [0.33387346 0.33125788 0.33486866]\n",
            " [0.33337628 0.33142646 0.33519726]\n",
            " [0.33397277 0.33111066 0.33491657]\n",
            " [0.33485457 0.33011549 0.33502994]\n",
            " [0.3333728  0.33159325 0.33503395]\n",
            " [0.33338246 0.33112306 0.33549447]\n",
            " [0.33338332 0.33108037 0.33553631]\n",
            " [0.33338024 0.33123295 0.3353868 ]\n",
            " [0.33338645 0.33092289 0.33569066]\n",
            " [0.33338789 0.33084936 0.33576275]\n",
            " [0.33338735 0.33087718 0.33573547]\n",
            " [0.33338031 0.33122993 0.33538977]\n",
            " [0.33338999 0.33074132 0.33586869]\n",
            " [0.33338285 0.331104   0.33551316]\n",
            " [0.33339076 0.3307016  0.33590764]\n",
            " [0.3333973  0.33149224 0.33511046]\n",
            " [0.33339257 0.33060735 0.33600008]\n",
            " [0.33339193 0.33064086 0.33596722]\n",
            " [0.33342585 0.33200209 0.33457205]\n",
            " [0.33347414 0.33335265 0.33317321]\n",
            " [0.33347942 0.33348613 0.33303445]\n",
            " [0.3334608  0.3328002  0.33373899]\n",
            " [0.33348238 0.33347013 0.3330475 ]\n",
            " [0.33350512 0.3344412  0.33205368]\n",
            " [0.33349754 0.33395953 0.33254292]\n",
            " [0.33350063 0.33402368 0.33247569]\n",
            " [0.33350681 0.33424555 0.33224764]\n",
            " [0.33385067 0.33470335 0.33144598]\n",
            " [0.33352459 0.33500018 0.33147523]\n",
            " [0.33354711 0.3352002  0.33125269]\n",
            " [0.3335441  0.33517339 0.33128251]\n",
            " [0.33357193 0.33542158 0.33100649]\n",
            " [0.33484811 0.33232457 0.33282732]\n",
            " [0.33408013 0.33433235 0.33158752]\n",
            " [0.33455737 0.33317034 0.3322723 ]\n",
            " [0.33519853 0.33141825 0.33338322]\n",
            " [0.33333333 0.33333333 0.33333333]\n",
            " [0.33336031 0.33328421 0.33335548]\n",
            " [0.33334607 0.33336037 0.33329356]\n",
            " [0.33338343 0.3332939  0.33332267]\n",
            " [0.33342859 0.33319039 0.33338102]\n",
            " [0.33344822 0.3331696  0.33338218]\n",
            " [0.33348116 0.33310032 0.33341852]\n",
            " [0.33342939 0.33329881 0.3332718 ]\n",
            " [0.33355477 0.33286511 0.33358013]\n",
            " [0.33355283 0.3329924  0.33345477]\n",
            " [0.33360146 0.33285057 0.33354797]\n",
            " [0.33340545 0.33347706 0.33311749]\n",
            " [0.33364941 0.33266497 0.33368563]\n",
            " [0.33359407 0.33278199 0.33362394]\n",
            " [0.33372881 0.33249705 0.33377414]\n",
            " [0.33375995 0.33243119 0.33380886]\n",
            " [0.33372034 0.33251497 0.3337647 ]\n",
            " [0.33376465 0.33262324 0.33361211]\n",
            " [0.33383559 0.33234409 0.33382032]\n",
            " [0.33385778 0.33222428 0.33391794]\n",
            " [0.33386774 0.33220321 0.33392905]\n",
            " [0.33349741 0.33266595 0.33383664]\n",
            " [0.3334013  0.33275051 0.33384819]\n",
            " [0.33340112 0.33272791 0.33387097]\n",
            " [0.33347837 0.33261619 0.33390544]\n",
            " [0.33374884 0.33234577 0.33390539]\n",
            " [0.33335331 0.33248468 0.33416201]\n",
            " [0.33334899 0.33267328 0.33397773]\n",
            " [0.33335074 0.33259746 0.3340518 ]\n",
            " [0.33391048 0.33211282 0.3339767 ]\n",
            " [0.33335232 0.33252824 0.33411944]\n",
            " [0.33351298 0.33241549 0.33407153]\n",
            " [0.33335605 0.33236356 0.33428039]\n",
            " [0.3333595  0.33220913 0.33443137]\n",
            " [0.33335965 0.33220245 0.3344379 ]\n",
            " [0.3333596  0.33220486 0.33443554]\n",
            " [0.33336116 0.33213427 0.33450457]\n",
            " [0.33336253 0.33207174 0.33456573]\n",
            " [0.33336012 0.33218104 0.33445884]\n",
            " [0.33335876 0.33224261 0.33439863]\n",
            " [0.33336141 0.33212279 0.3345158 ]\n",
            " [0.3333655  0.33193557 0.33469893]\n",
            " [0.33337756 0.33256496 0.33405748]\n",
            " [0.33336861 0.33232342 0.33430797]\n",
            " [0.33343187 0.33418519 0.33238294]\n",
            " [0.3333774  0.33243398 0.33418863]\n",
            " [0.33351221 0.33421698 0.33227081]\n",
            " [0.33338244 0.33246612 0.33415144]\n",
            " [0.3333988  0.3328398  0.3337614 ]\n",
            " [0.33341776 0.33339271 0.33318952]\n",
            " [0.3334164  0.33328236 0.33330125]\n",
            " [0.33345203 0.33436123 0.33218674]\n",
            " [0.33336609 0.3319183  0.33471561]\n",
            " [0.33375809 0.33384    0.33240191]\n",
            " [0.33337362 0.33155423 0.33507214]\n",
            " [0.33344809 0.33432678 0.33222512]\n",
            " [0.33347233 0.33453917 0.3319885 ]\n",
            " [0.33405346 0.33321687 0.33272967]\n",
            " [0.33348198 0.33462395 0.33189407]\n",
            " [0.33347514 0.33456383 0.33196103]\n",
            " [0.33358811 0.3344365  0.33197539]\n",
            " [0.33464479 0.33164335 0.33371186]\n",
            " [0.33354434 0.33459122 0.33186444]\n",
            " [0.33346017 0.3342651  0.33227473]\n",
            " [0.33348228 0.33462663 0.33189108]\n",
            " [0.33356779 0.33462216 0.33181005]\n",
            " [0.33518869 0.32953208 0.33527923]\n",
            " [0.33441698 0.33255437 0.33302865]\n",
            " [0.33511796 0.32955806 0.33532398]\n",
            " [0.33434229 0.33282686 0.33283085]\n",
            " [0.33526745 0.32924165 0.33549089]\n",
            " [0.33530439 0.32916347 0.33553214]\n",
            " [0.33491689 0.33122106 0.33386205]\n",
            " [0.33534649 0.32947537 0.33517814]\n",
            " [0.33533889 0.32909045 0.33557066]\n",
            " [0.33545444 0.32884586 0.3356997 ]\n",
            " [0.33540163 0.32895764 0.33564073]\n",
            " [0.33522552 0.32933042 0.33544406]\n",
            " [0.33531649 0.32913786 0.33554565]\n",
            " [0.3354726  0.32939675 0.33513065]\n",
            " [0.33547836 0.32879521 0.33572643]\n",
            " [0.3354718  0.32880909 0.3357191 ]\n",
            " [0.33388785 0.3308182  0.33529395]\n",
            " [0.33488396 0.32995369 0.33516235]\n",
            " [0.33455866 0.33014217 0.33529917]\n",
            " [0.33339184 0.33064529 0.33596287]\n",
            " [0.33477646 0.32993035 0.33529318]\n",
            " [0.33429349 0.33029205 0.33541447]\n",
            " [0.33338494 0.33099891 0.33561615]\n",
            " [0.33570066 0.3283246  0.33597474]\n",
            " [0.33524404 0.32929122 0.33546474]\n",
            " [0.33405242 0.33043253 0.33551505]\n",
            " [0.33339994 0.33021325 0.33638681]\n",
            " [0.33339997 0.33021159 0.33638843]\n",
            " [0.33340075 0.33016889 0.33643035]\n",
            " [0.33359152 0.33085664 0.33555183]\n",
            " [0.3334027  0.33006137 0.33653593]\n",
            " [0.33340332 0.33002704 0.33656964]\n",
            " [0.33388437 0.33044928 0.33566635]\n",
            " [0.33340455 0.32995808 0.33663737]\n",
            " [0.33333333 0.33333333 0.33333333]\n",
            " [0.33333416 0.33329918 0.33336666]\n",
            " [0.3333348  0.33327306 0.33339214]\n",
            " [0.33333559 0.33325698 0.33340743]\n",
            " [0.33333942 0.33331078 0.3333498 ]\n",
            " [0.33334224 0.33335039 0.33330737]\n",
            " [0.3333381  0.33313606 0.33352584]\n",
            " [0.33334577 0.33335596 0.33329827]\n",
            " [0.33333949 0.33313343 0.33352708]\n",
            " [0.33334799 0.33331398 0.33333804]\n",
            " [0.33335179 0.33339353 0.33325468]\n",
            " [0.3333443  0.33311479 0.33354091]\n",
            " [0.33336293 0.3335877  0.33304937]\n",
            " [0.33336465 0.33360247 0.33303289]\n",
            " [0.33336342 0.3335919  0.33304468]\n",
            " [0.33336426 0.33358847 0.33304727]\n",
            " [0.33337151 0.33366163 0.33296686]\n",
            " [0.33352233 0.33336511 0.33311256]\n",
            " [0.33334593 0.33285972 0.33379434]\n",
            " [0.33336764 0.33341703 0.33321532]\n",
            " [0.33340179 0.33373903 0.33285918]\n",
            " [0.33338773 0.33380182 0.33281044]\n",
            " [0.33338963 0.33381825 0.33279211]\n",
            " [0.33339259 0.33384385 0.33276356]\n",
            " [0.33358861 0.3334075  0.3330039 ]\n",
            " [0.33403313 0.33193887 0.334028  ]\n",
            " [0.33379065 0.33292525 0.3332841 ]\n",
            " [0.33369189 0.33323211 0.333076  ]\n",
            " [0.3335453  0.33363128 0.33282343]\n",
            " [0.3339216  0.33264223 0.33343617]\n",
            " [0.33372482 0.33323916 0.33303603]\n",
            " [0.33412851 0.33200206 0.33386943]\n",
            " [0.33423677 0.33144136 0.33432187]\n",
            " [0.33424644 0.33157444 0.33417912]\n",
            " [0.3340298  0.33186044 0.33410976]\n",
            " [0.33342738 0.33410034 0.33247228]\n",
            " [0.33427694 0.33169815 0.33402491]\n",
            " [0.33436802 0.33114497 0.33448701]\n",
            " [0.33364467 0.33211068 0.33424466]\n",
            " [0.33441051 0.3312669  0.33432259]\n",
            " [0.3344393  0.33120576 0.33435494]\n",
            " [0.33437079 0.3311391  0.3344901 ]\n",
            " [0.33446112 0.330948   0.33459088]\n",
            " [0.33416306 0.33156719 0.33426975]\n",
            " [0.33453715 0.33078715 0.33467571]\n",
            " [0.33454482 0.33077092 0.33468427]\n",
            " [0.3344971  0.33087188 0.33463102]\n",
            " [0.3342617  0.33136989 0.33436841]\n",
            " [0.33420181 0.33139334 0.33440484]\n",
            " [0.33420547 0.33136154 0.33443299]\n",
            " [0.33407142 0.33142628 0.3345023 ]\n",
            " [0.33337191 0.33163593 0.33499216]\n",
            " [0.33339854 0.33207153 0.33452993]\n",
            " [0.33336606 0.33190988 0.33472406]\n",
            " [0.33336905 0.33177068 0.33486027]\n",
            " [0.33337102 0.33167795 0.33495104]\n",
            " [0.3335368  0.33181318 0.33465002]\n",
            " [0.33337501 0.3314875  0.33513749]\n",
            " [0.33336915 0.33176571 0.33486514]\n",
            " [0.33342777 0.33315125 0.33342099]\n",
            " [0.33337314 0.33157715 0.33504971]\n",
            " [0.33337498 0.33148918 0.33513584]\n",
            " [0.33340283 0.33230803 0.33428915]\n",
            " [0.33337965 0.33126201 0.33535834]\n",
            " [0.3333814  0.3311761  0.3354425 ]\n",
            " [0.33343329 0.33300118 0.33356553]\n",
            " [0.33338308 0.33109245 0.33552447]\n",
            " [0.33338391 0.33105081 0.33556527]\n",
            " [0.33337612 0.33143399 0.33518989]\n",
            " [0.33344452 0.33314527 0.33341022]\n",
            " [0.33347119 0.33419109 0.33233772]\n",
            " [0.33339544 0.33176902 0.33483554]\n",
            " [0.33338387 0.33105277 0.33556335]\n",
            " [0.33346213 0.33353708 0.33300079]\n",
            " [0.33347823 0.33419625 0.33232552]\n",
            " [0.3335048  0.33482513 0.33167007]\n",
            " [0.33345757 0.33318611 0.33335631]\n",
            " [0.33344155 0.33263682 0.33392163]\n",
            " [0.33346797 0.33343697 0.33309506]\n",
            " [0.33353532 0.33509535 0.33136934]\n",
            " [0.33343431 0.33230031 0.33426538]\n",
            " [0.33353986 0.33513576 0.33132437]\n",
            " [0.3335429  0.33516276 0.33129434]\n",
            " [0.33423113 0.33355151 0.33221736]\n",
            " [0.33354031 0.33513973 0.33131996]\n",
            " [0.33366145 0.33496865 0.3313699 ]\n",
            " [0.33348419 0.33353982 0.33297599]\n",
            " [0.33461047 0.33268178 0.33270775]\n",
            " [0.33351896 0.33495029 0.33153075]\n",
            " [0.33541512 0.33029252 0.33429236]\n",
            " [0.33578558 0.32875917 0.33545524]\n",
            " [0.3358591  0.32839678 0.33574412]\n",
            " [0.33450153 0.33313041 0.33236806]\n",
            " [0.33564987 0.32962277 0.33472736]\n",
            " [0.33597947 0.32776079 0.33625974]\n",
            " [0.33559753 0.32854293 0.33585954]\n",
            " [0.33513557 0.3314954  0.33336903]\n",
            " [0.33520527 0.33132248 0.33347225]\n",
            " [0.33567213 0.32838499 0.33594287]\n",
            " [0.33611188 0.32764208 0.33624604]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mlPUVj9GP2Ni"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}